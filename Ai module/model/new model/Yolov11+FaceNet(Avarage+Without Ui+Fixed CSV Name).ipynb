{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3403390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from scipy.spatial.distance import cosine\n",
    "import logging\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ba9dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:48:26,076 - INFO - Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load YAML configuration\n",
    "with open(\"config(avg).yaml\", \"r\") as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "def hex_to_bgr(hex_color):\n",
    "    \"\"\"Convert hex color to BGR tuple\"\"\"\n",
    "    hex_color = hex_color.lstrip(\"#\")\n",
    "    r, g, b = int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)\n",
    "    return (b, g, r)\n",
    "\n",
    "# Convert yolo_input_size list to tuple\n",
    "CONFIG[\"yolo_input_size\"] = tuple(CONFIG[\"yolo_input_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ff40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"Load YOLOv11 and FaceNet\"\"\"\n",
    "    try:\n",
    "        yolo = YOLO(CONFIG[\"model_paths\"][\"yolo\"]).eval()\n",
    "        facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        facenet.to(device)\n",
    "        return yolo, facenet, device\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading models: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2245d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(frame, model):\n",
    "    \"\"\"Detect faces using YOLO\"\"\"\n",
    "    try:\n",
    "        results = model(frame)\n",
    "        boxes = []\n",
    "        for result in results[0].boxes:\n",
    "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
    "            conf = float(result.conf[0])\n",
    "            cls = int(result.cls[0])\n",
    "            if conf > CONFIG[\"detection_threshold\"] and cls == 0:  # Class 0 = Person\n",
    "                boxes.append((x1, y1, x2, y2))\n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detecting faces: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601e5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(face_imgs, model, device):\n",
    "    \"\"\"Extract 512D embeddings from multiple face images (batch processing)\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ])\n",
    "    \n",
    "    if not face_imgs:  # Handle empty face_imgs case\n",
    "        logging.debug(\"No face images provided for embedding extraction\")\n",
    "        return np.array([])  # Return empty NumPy array\n",
    "    \n",
    "    try:\n",
    "        tensors = torch.stack([transform(Image.fromarray(face_img)) for face_img in face_imgs]).to(device)\n",
    "        with torch.no_grad():\n",
    "            embs = model(tensors)\n",
    "        embs = embs / embs.norm(p=2, dim=1, keepdim=True)\n",
    "        return embs.cpu().numpy()  # Still return NumPy array for compatibility\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting embeddings: {e}\")\n",
    "        return np.array([])  # Return empty NumPy array on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2ba7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_scores(embedding, stored, device):\n",
    "    \"\"\"Compare new embedding with each person's stored embeddings using GPU\"\"\"\n",
    "    similarities = {}\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32, device=device)\n",
    "    embedding_tensor = embedding_tensor / embedding_tensor.norm(p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    for name, embeds in stored.items():\n",
    "        embeds_tensor = torch.tensor(embeds, dtype=torch.float32, device=device)\n",
    "        embeds_tensor = embeds_tensor / embeds_tensor.norm(p=2, dim=-1, keepdim=True)\n",
    "        scores = torch.matmul(embeds_tensor, embedding_tensor).cpu().numpy()\n",
    "        similarities[name] = round(float(np.mean(scores)), 4)\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c91fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image_box(frame, x1, y1, x2, y2, name, box_img):\n",
    "    \"\"\"Draw bounding box and label on frame\"\"\"\n",
    "    box_width, box_height = x2 - x1, y2 - y1\n",
    "    overlay_image_alpha(frame, box_img, x1, y1, (box_width, box_height))\n",
    "\n",
    "    try:\n",
    "        person_name, student_id = name.split(\"_\", 1)\n",
    "    except ValueError:\n",
    "        person_name, student_id = name, \"N/A\"\n",
    "\n",
    "    label = f\"Name: {person_name}\\nID: {student_id}\"\n",
    "\n",
    "    font_path = CONFIG[\"model_paths\"].get(\"font_path\", \"\")\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, 20)\n",
    "    except Exception:\n",
    "        logging.warning(\"Font not found. Falling back to default.\")\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    img_pil = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    lines = label.split(\"\\n\")\n",
    "    line_height = 25\n",
    "    total_text_height = len(lines) * line_height\n",
    "    label_x, label_y = x1, y1 - total_text_height - 1\n",
    "\n",
    "    hex_color = CONFIG[\"label_color\"]\n",
    "    label_color = tuple(int(hex_color[i:i+2], 16) for i in (1, 3, 5))\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        draw.text((label_x, label_y + i * line_height), line, font=font, fill=label_color)\n",
    "\n",
    "    return np.array(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4859f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_image_alpha(background, overlay, x, y, overlay_size=None):\n",
    "    \"\"\"Draw image with transparency\"\"\"\n",
    "    if overlay_size:\n",
    "        overlay = cv2.resize(overlay, overlay_size)\n",
    "    h, w = overlay.shape[:2]\n",
    "    if y + h > background.shape[0] or x + w > background.shape[1]:\n",
    "        return\n",
    "    overlay_img = overlay[:, :, :3]\n",
    "    mask = overlay[:, :, 3:] / 255.0\n",
    "    background_crop = background[y:y+h, x:x+w]\n",
    "    background[y:y+h, x:x+w] = (1 - mask) * background_crop + mask * overlay_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c9a57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_csv(stored_embeddings):\n",
    "    \"\"\"Create CSV with all students marked as Absent\"\"\"\n",
    "    csv_data = []\n",
    "    date = time.strftime(\"%Y-%m-%d\")\n",
    "    for name in stored_embeddings.keys():\n",
    "        csv_data.append([name, \"Absent\", date, \"\"])\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e3a84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_csv_filename():\\n    Generate CSV filename based on session time\\n    now = time.localtime()\\n    hour = now.tm_hour\\n    if hour == 0:  # Midnight: belongs to previous day\\'s 11PM-12:59AM block\\n        prev_day = time.localtime(time.mktime(now) - 86400)\\n        date_str = time.strftime(\"%Y-%m-%d\", prev_day)\\n        start_hour = 23\\n    else:\\n        date_str = time.strftime(\"%Y-%m-%d\", now)\\n        if hour % 2 == 0:\\n            start_hour = hour - 1\\n        else:\\n            start_hour = hour\\n    am_pm = \"AM\" if start_hour < 12 else \"PM\"\\n    display_hour = start_hour if start_hour <= 12 else start_hour - 12\\n    if display_hour == 0:\\n        display_hour = 12\\n    return f\"{date_str}_session_{display_hour}{am_pm}.csv\"\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_csv_filename():\n",
    "    Generate CSV filename based on session time\n",
    "    now = time.localtime()\n",
    "    hour = now.tm_hour\n",
    "    if hour == 0:  # Midnight: belongs to previous day's 11PM-12:59AM block\n",
    "        prev_day = time.localtime(time.mktime(now) - 86400)\n",
    "        date_str = time.strftime(\"%Y-%m-%d\", prev_day)\n",
    "        start_hour = 23\n",
    "    else:\n",
    "        date_str = time.strftime(\"%Y-%m-%d\", now)\n",
    "        if hour % 2 == 0:\n",
    "            start_hour = hour - 1\n",
    "        else:\n",
    "            start_hour = hour\n",
    "    am_pm = \"AM\" if start_hour < 12 else \"PM\"\n",
    "    display_hour = start_hour if start_hour <= 12 else start_hour - 12\n",
    "    if display_hour == 0:\n",
    "        display_hour = 12\n",
    "    return f\"{date_str}_session_{display_hour}{am_pm}.csv\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4c2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_filename():\n",
    "    return \"Attendance Sheet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cca5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_attendance(name, attendance_logged, csv_data):\n",
    "    \"\"\"Mark student as present in CSV if not already marked\"\"\"\n",
    "    if name == \"Unknown\":\n",
    "        return\n",
    "    if name in attendance_logged:\n",
    "        return\n",
    "    date, clock = time.strftime(\"%Y-%m-%d\"), time.strftime(\"%H:%M:%S\")\n",
    "    for row in csv_data:\n",
    "        if row[0] == name:\n",
    "            row[1] = \"Attend\"\n",
    "            row[2] = date\n",
    "            row[3] = clock\n",
    "    attendance_logged.add(name)\n",
    "    logging.info(f\"[{date} {clock}] Marked: {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f78b19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_attendance_session():\n",
    "    \"\"\"Main function to run the attendance session with only camera feed and smaller FPS\"\"\"\n",
    "    yolo, facenet, device = load_models()\n",
    "\n",
    "    with open(CONFIG[\"model_paths\"][\"embeddings\"], \"rb\") as f:\n",
    "        stored_embeddings = pickle.load(f)\n",
    "\n",
    "    box_img = cv2.imread(CONFIG[\"model_paths\"][\"box_img\"], cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    vote_tracker = {}\n",
    "    attendance_logged = set()\n",
    "    csv_data = initialize_csv(stored_embeddings)\n",
    "\n",
    "    ### Hikvision\n",
    "    \"\"\"\n",
    "    # Initialize RTSP camera\n",
    "    rtsp_url = \"rtsp://admin:Starthassan%402002@192.168.1.64/Streaming/Channels/101\"\n",
    "    cap = cv2.VideoCapture(rtsp_url)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        logging.error(\"Could not open camera. Check RTSP URL or camera connection.\")\n",
    "        return\n",
    "    # Use native camera resolution\n",
    "    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 4)\n",
    "    \"\"\"\n",
    "####\n",
    "\n",
    "#### webcam\n",
    "    cap = cv2.VideoCapture(0)  \n",
    "    if not cap.isOpened():\n",
    "        logging.error(\"Could not open webcam. Please check your device connection.\")\n",
    "        return\n",
    "####\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    logging.info(f\"Camera resolution: {width}x{height}\")\n",
    "\n",
    "    # Create window with normal size\n",
    "    cv2.namedWindow(\"Face Attendance\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Optional: Set a fixed display size (uncomment to use)\n",
    "    # display_width, display_height = 1280, 720\n",
    "    # cv2.resizeWindow(\"Face Attendance\", display_width, display_height)\n",
    "\n",
    "    yolo_input_width, yolo_input_height = CONFIG[\"yolo_input_size\"]\n",
    "    session_start_time = time.time()\n",
    "    logging.info(\"Press 'q' to end session.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Check session timeout\n",
    "            elapsed_time = time.time() - session_start_time\n",
    "            if elapsed_time > CONFIG[\"session_timeout\"]:\n",
    "                logging.info(f\"Session timeout reached ({CONFIG['session_timeout']} seconds). Exiting session...\")\n",
    "                break\n",
    "\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                logging.warning(\"Failed to read frame from camera\")\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()  # Start time for FPS calculation\n",
    "            logging.debug(f\"Frame shape: {frame.shape}\")\n",
    "\n",
    "            # Resize frame for YOLO while preserving aspect ratio\n",
    "            orig_width, orig_height = frame.shape[1], frame.shape[0]\n",
    "            yolo_frame = cv2.resize(frame, (yolo_input_width, yolo_input_height))\n",
    "\n",
    "            # Detect faces on resized frame\n",
    "            boxes = detect_faces(yolo_frame, yolo)\n",
    "\n",
    "            # Scale bounding boxes back to original frame size\n",
    "            scale_x = orig_width / yolo_input_width\n",
    "            scale_y = orig_height / yolo_input_height\n",
    "            boxes = [(int(x1 * scale_x), int(y1 * scale_y), int(x2 * scale_x), int(y2 * scale_y))\n",
    "                     for x1, y1, x2, y2 in boxes]\n",
    "\n",
    "            # Extract face images from original frame\n",
    "            face_imgs = []\n",
    "            for x1, y1, x2, y2 in boxes:\n",
    "                face = frame[max(0, y1):y2, max(0, x1):x2]\n",
    "                if face.size == 0:\n",
    "                    continue\n",
    "                face_imgs.append(face)\n",
    "\n",
    "            # Extract embeddings\n",
    "            embeddings = extract_embeddings(face_imgs, facenet, device)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                for i, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "                    embedding = embeddings[i]\n",
    "                    similarities = get_similarity_scores(embedding, stored_embeddings, device)\n",
    "\n",
    "                    logging.info(\"\\n--- Face Detected ---\")\n",
    "                    for name, score in sorted(similarities.items(), key=lambda x: x[1], reverse=True):\n",
    "                        logging.info(f\"{name}: {score:.4f}\")\n",
    "\n",
    "                    best_match = max(similarities, key=similarities.get)\n",
    "                    best_score = similarities[best_match]\n",
    "\n",
    "                    if best_score >= CONFIG[\"similarity_threshold\"]:\n",
    "                        vote_tracker[best_match] = vote_tracker.get(best_match, 0) + 1\n",
    "                        if vote_tracker[best_match] >= CONFIG[\"frame_threshold\"]:\n",
    "                            log_attendance(best_match, attendance_logged, csv_data)\n",
    "\n",
    "                    name = best_match if best_score >= CONFIG[\"similarity_threshold\"] else \"Unknown\"\n",
    "                    frame = draw_image_box(frame, x1, y1, x2, y2, name, box_img)\n",
    "\n",
    "            # Draw smaller FPS on the frame\n",
    "            end_time = time.time()\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            cv2.putText(frame, f\"FPS: {fps:.2f}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            #  Resize frame for display (uncomment )\n",
    "            # frame = cv2.resize(frame, (display_width, display_height))\n",
    "\n",
    "            # Display the raw camera feed with bounding boxes, labels, and FPS\n",
    "            cv2.imshow(\"Face Attendance\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q') or key == ord('Q'):\n",
    "                logging.info(\"Exit key 'Q' pressed. Exiting session...\")\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \"\"\"\n",
    "        session_end_time = time.time()\n",
    "        duration = time.strftime(\"%H:%M:%S\", time.gmtime(session_end_time - session_start_time))\n",
    "        csv_data.append([\"Total Duration\", \"\", \"\", \"\", duration])\n",
    "        \"\"\"\n",
    "        csv_file = get_csv_filename()\n",
    "        with open(csv_file, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            data_rows = [row for row in csv_data if row[1] != \"\" and row[1] != \"N/A\"]\n",
    "            footer_rows = [row for row in csv_data if row[1] == \"\" or row[1] == \"N/A\"]\n",
    "            data_rows.sort(key=lambda x: x[1])\n",
    "            writer.writerow([\"Student Name\", \"ID\", \"Status\", \"Date\", \"Time\"])\n",
    "            for row in data_rows:\n",
    "                name, student_id = row[0].split(\"_\", 1) if \"_\" in row[0] else (row[0], \"N/A\")\n",
    "                writer.writerow([name, student_id, row[1], row[2], row[3]])\n",
    "            writer.writerows(footer_rows)\n",
    "\n",
    "        logging.info(f\"Session ended. Attendance saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea08fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:48:30,380 - INFO - Camera resolution: 640x480\n",
      "2025-06-13 16:48:30,396 - INFO - Press 'q' to end session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 576x1024 (no detections), 201.0ms\n",
      "Speed: 6.3ms preprocess, 201.0ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 152.9ms\n",
      "Speed: 4.5ms preprocess, 152.9ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 154.9ms\n",
      "Speed: 4.8ms preprocess, 154.9ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 170.4ms\n",
      "Speed: 7.0ms preprocess, 170.4ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 160.3ms\n",
      "Speed: 5.2ms preprocess, 160.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 149.9ms\n",
      "Speed: 5.1ms preprocess, 149.9ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 154.8ms\n",
      "Speed: 6.9ms preprocess, 154.8ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 157.0ms\n",
      "Speed: 6.4ms preprocess, 157.0ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 172.9ms\n",
      "Speed: 5.8ms preprocess, 172.9ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 154.3ms\n",
      "Speed: 5.0ms preprocess, 154.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 157.1ms\n",
      "Speed: 4.8ms preprocess, 157.1ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 166.8ms\n",
      "Speed: 4.2ms preprocess, 166.8ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 164.1ms\n",
      "Speed: 7.2ms preprocess, 164.1ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 195.2ms\n",
      "Speed: 4.9ms preprocess, 195.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 160.6ms\n",
      "Speed: 7.8ms preprocess, 160.6ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 166.8ms\n",
      "Speed: 4.5ms preprocess, 166.8ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 249.1ms\n",
      "Speed: 4.6ms preprocess, 249.1ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 200.6ms\n",
      "Speed: 5.4ms preprocess, 200.6ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 193.6ms\n",
      "Speed: 6.1ms preprocess, 193.6ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 189.3ms\n",
      "Speed: 5.6ms preprocess, 189.3ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 180.4ms\n",
      "Speed: 6.9ms preprocess, 180.4ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 164.8ms\n",
      "Speed: 5.5ms preprocess, 164.8ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 176.3ms\n",
      "Speed: 5.2ms preprocess, 176.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 182.9ms\n",
      "Speed: 6.2ms preprocess, 182.9ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 165.4ms\n",
      "Speed: 7.0ms preprocess, 165.4ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 175.2ms\n",
      "Speed: 6.4ms preprocess, 175.2ms inference, 1.8ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 160.0ms\n",
      "Speed: 5.5ms preprocess, 160.0ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 175.6ms\n",
      "Speed: 7.2ms preprocess, 175.6ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 165.6ms\n",
      "Speed: 4.7ms preprocess, 165.6ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 162.7ms\n",
      "Speed: 5.1ms preprocess, 162.7ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 160.2ms\n",
      "Speed: 6.0ms preprocess, 160.2ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 159.2ms\n",
      "Speed: 5.2ms preprocess, 159.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 156.4ms\n",
      "Speed: 6.2ms preprocess, 156.4ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 166.2ms\n",
      "Speed: 5.0ms preprocess, 166.2ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 161.5ms\n",
      "Speed: 5.5ms preprocess, 161.5ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 160.3ms\n",
      "Speed: 5.3ms preprocess, 160.3ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 159.1ms\n",
      "Speed: 5.0ms preprocess, 159.1ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 161.8ms\n",
      "Speed: 6.1ms preprocess, 161.8ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 159.3ms\n",
      "Speed: 6.5ms preprocess, 159.3ms inference, 0.8ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 172.8ms\n",
      "Speed: 5.4ms preprocess, 172.8ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 164.6ms\n",
      "Speed: 5.9ms preprocess, 164.6ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 170.4ms\n",
      "Speed: 5.3ms preprocess, 170.4ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 225.2ms\n",
      "Speed: 5.5ms preprocess, 225.2ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 179.1ms\n",
      "Speed: 8.3ms preprocess, 179.1ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 176.3ms\n",
      "Speed: 5.2ms preprocess, 176.3ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 177.0ms\n",
      "Speed: 5.7ms preprocess, 177.0ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 167.9ms\n",
      "Speed: 6.6ms preprocess, 167.9ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 175.1ms\n",
      "Speed: 4.9ms preprocess, 175.1ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 342.5ms\n",
      "Speed: 8.6ms preprocess, 342.5ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 250.2ms\n",
      "Speed: 7.1ms preprocess, 250.2ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 237.5ms\n",
      "Speed: 8.6ms preprocess, 237.5ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 234.2ms\n",
      "Speed: 8.3ms preprocess, 234.2ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 253.0ms\n",
      "Speed: 6.2ms preprocess, 253.0ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 203.0ms\n",
      "Speed: 6.5ms preprocess, 203.0ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 206.5ms\n",
      "Speed: 6.7ms preprocess, 206.5ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 576x1024 (no detections), 187.7ms\n",
      "Speed: 5.6ms preprocess, 187.7ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:48:44,491 - INFO - Session ended. Attendance saved to: Attendance Sheet.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mrun_attendance_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 73\u001b[0m, in \u001b[0;36mrun_attendance_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m yolo_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (yolo_input_width, yolo_input_height))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Detect faces on resized frame\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Scale bounding boxes back to original frame size\u001b[39;00m\n\u001b[0;32m     76\u001b[0m scale_x \u001b[38;5;241m=\u001b[39m orig_width \u001b[38;5;241m/\u001b[39m yolo_input_width\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(frame, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect faces using YOLO\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:552\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:218\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:329\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 329\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    168\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    169\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    172\u001b[0m )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\autobackend.py:592\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 592\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:115\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:133\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:154\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 154\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    155\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 301\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 301\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_attendance_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
